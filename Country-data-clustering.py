# -*- coding: utf-8 -*-
"""B21EE056_B21EE060_PRML_Minor_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AES7IgbL0dwMi6zSwNI1nP2ukAHWaLjy
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("Country-data.csv")
df

# Getting the counrtry names list
#print(df)
country_list = df['country'].tolist()
print(country_list)

df_original = df.copy()
print(df_original)

#Getting dimensions of dataframe
df.shape

df.describe()

#Checking for the NULL values in the dataset
df.isnull().any()

#Getting the heapmap to study the co-relation between he features
corr_df = df.corr()
sns.heatmap(corr_df, annot = True, cmap="seismic_r")
plt.show()

from random import sample
import matplotlib.image as mpimg
import seaborn as sns
import plotly.express as px
import plotly.io as pio
!pip install -U kaleido
import kaleido
pio.renderers

#Creating country indexing list
c_lst = []
for i in range(0,167):
  c_lst.append(i)
print(c_lst)

for i in df.drop('country', axis=1).columns:
    fig = px.choropleth(df, locationmode='country names', locations='country',title=i+' per country (World)',color=i,color_continuous_scale="Reds")
    #fig.write_html(f"Interactive-plots_World_{i}.html")
    fig.update_geos(fitbounds="locations", visible=True)
    fig.show(engine='kaleido')

sns.jointplot(data=df, x=df['exports'], y=df['imports'])

sns.jointplot(data=df, x=df['health'], y=df['life_expec'])

sns.jointplot(data=df, x=df['inflation'], y=df['income'])

sns.jointplot(data=df, x=df['child_mort'], y=df['life_expec'])

sns.jointplot(data=df, x=df['income'], y=df['life_expec'])

country_names = df['country']
df = df.drop(['country'], axis=1)

from sklearn.preprocessing import StandardScaler
scale_data = StandardScaler()
col = df.columns
df_scaled = pd.DataFrame(scale_data.fit_transform(df), columns=col)
df_scaled

#Getting the heapmap to study the co-relation between the features after scaling the data
corr_df = df_scaled.corr()
sns.heatmap(corr_df, annot = True, cmap="seismic_r")
plt.show()

"""Applying PCA"""

from sklearn.decomposition import PCA
pca = PCA()
pcadf = pca.fit_transform(df_scaled)
pca_df = pd.DataFrame(data = pcadf, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9'])
pca_df

exvar = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(exvar * 100)
print(cumulative_variance)

# Plotting the Scree Plot to find the number of required components:
plt.plot(cumulative_variance, marker = 'o')
plt.title('Scree Plot')
plt.xlabel('No of Principal Components -->')
plt.ylabel('Cumulative Variance -->')
plt.show()

# from the scree plot, we got the number of components = 5, so we will drop all other columns
pca_df = pca_df.drop(['PC6','PC7','PC8','PC9'], axis = 1)
pca_df

plt.scatter(pca_df.PC1, pca_df.PC2)
plt.title('PC1 vs PC2')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# Visualizing the PCA for the obtained n_components as 5.
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Apply PCA to the scaled data
pca = PCA(n_components=5)
pca.fit(df_scaled)
scores = pca.transform(df_scaled)

# Create a scatter plot matrix of the scores on the first five principal components
sns.pairplot(pd.DataFrame(scores[:, 0:5]))
plt.show()

"""Applying KMeans on PCA data:"""

# finding the best value of K by using the elbow method:
from sklearn.cluster import KMeans
inertia_val = []
k_val = [1,2,3,4,5,6,7,8,9,10]
for i in range(1, 11):
  kmean = KMeans(n_clusters = i)
  kmean.fit(pca_df)
  inertia_val.append(kmean.inertia_)

plt.plot(k_val, inertia_val, marker = 'o')
plt.title("Elbow Method:")
plt.show()

# We can see that the elbow is coming at k = 4, so we will take k = 4
kmean = KMeans(n_clusters = 4)
kmean.fit(pca_df)
kmeans_label_alotted = kmean.labels_
centroids_pca = kmean.cluster_centers_
print(kmeans_label_alotted)

# Getting the country list
cl1 = []
cl2 = []
cl3 = []
cl4 = []
for i in range(len(kmeans_label_alotted)):
  if(kmeans_label_alotted[i]==0):
    cl1.append(country_list[i])
  if(kmeans_label_alotted[i]==1):
    cl2.append(country_list[i])
  if(kmeans_label_alotted[i]==2):
    cl3.append(country_list[i])
  if(kmeans_label_alotted[i]==3):
    cl4.append(country_list[i])

print("Countries in cluster 1",cl1 )
print("Countries in cluster 2",cl2 )
print("Countries in cluster 3",cl3 )
print("Countries in cluster 4",cl4 )

def count_label(arr, cla):
  cnt = 0
  for i in arr:
    if i == cla:
      cnt += 1
  return cnt

cnt0 = count_label(kmeans_label_alotted, 0)
cnt1 = count_label(kmeans_label_alotted, 1)
cnt2 = count_label(kmeans_label_alotted, 2)
cnt3 = count_label(kmeans_label_alotted, 3)

tot_cnt = cnt0 + cnt1 + cnt2 + cnt3
print(cnt0, cnt1, cnt2, cnt3)
print(cnt0*100/tot_cnt, cnt1*100/tot_cnt, cnt2*100/tot_cnt, cnt3*100/tot_cnt)

colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'pink']
plt.figure(figsize=(8, 6))
for i in range(4):
    x_arr = pca_df.iloc[kmeans_label_alotted == i, 0]
    y_arr = pca_df.iloc[kmeans_label_alotted == i, 1]
    plt.scatter(x_arr, y_arr, s=30, c=colors[i])

x_cent = centroids_pca[:, 0]
y_cent = centroids_pca[:, 1]
plt.scatter(x_cent, y_cent, marker='*', c='black', s = 200)
plt.title('KMeans Clustering on PCA dataset:')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

"""KMeans without PCA on the Standard Scaled dataset:"""

# finding the best value of K by using the elbow method:
from sklearn.cluster import KMeans
inertia_val = []
k_val = [1,2,3,4,5,6,7,8,9,10]
for i in range(1, 11):
  kmean = KMeans(n_clusters = i)
  kmean.fit(df_scaled)
  inertia_val.append(kmean.inertia_)

plt.plot(k_val, inertia_val, marker = 'o')
plt.title("Elbow Method:")
plt.show()

# We can see that the elbow is coming at k = 4, so we will take k = 4
kmean = KMeans(n_clusters = 4)
kmean.fit(df_scaled)
label_alotted = kmean.labels_
centroids_scaled = kmean.cluster_centers_
print(label_alotted)

cnt0 = count_label(label_alotted, 0)
cnt1 = count_label(label_alotted, 1)
cnt2 = count_label(label_alotted, 2)
cnt3 = count_label(label_alotted, 3)

tot_cnt = cnt0 + cnt1 + cnt2 + cnt3
print(cnt0, cnt1, cnt2, cnt3)
print(cnt0*100/tot_cnt, cnt1*100/tot_cnt, cnt2*100/tot_cnt, cnt3*100/tot_cnt)

colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'pink']
plt.figure(figsize=(8, 6))
for i in range(4):
    x_arr = df_scaled.iloc[label_alotted == i, 0]
    y_arr = df_scaled.iloc[label_alotted == i, 1]
    plt.scatter(x_arr, y_arr, s=30, c=colors[i])

x_cent = centroids_scaled[:, 0]
y_cent = centroids_scaled[:, 1]
plt.scatter(x_cent, y_cent, marker='*', c='black', s = 200)
plt.title('KMeans Clustering on Scaled dataset:')
plt.xlabel('child_mort')
plt.ylabel('exports')
plt.show()

colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'pink']
plt.figure(figsize=(8, 6))
for i in range(4):
    x_arr = df_scaled.iloc[label_alotted == i, 4]
    y_arr = df_scaled.iloc[label_alotted == i, 8]
    plt.scatter(x_arr, y_arr, s=30, c=colors[i])

x_cent = centroids_scaled[:, 4]
y_cent = centroids_scaled[:, 8]
plt.scatter(x_cent, y_cent, marker='*', c='black', s = 200)
plt.title('KMeans Clustering on Scaled dataset:')
plt.xlabel('Income')
plt.ylabel('GDPP')
plt.show()

colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'pink']
plt.figure(figsize=(8, 6))
for i in range(4):
    x_arr = df_scaled.iloc[label_alotted == i, 5]
    y_arr = df_scaled.iloc[label_alotted == i, 8]
    plt.scatter(x_arr, y_arr, s=30, c=colors[i])

x_cent = centroids_scaled[:, 5]
y_cent = centroids_scaled[:, 8]
plt.scatter(x_cent, y_cent, marker='*', c='black', s = 200)
plt.title('KMeans Clustering on Scaled dataset:')
plt.xlabel('Inflation')
plt.ylabel('GDPP')
plt.show()

from sklearn.metrics import silhouette_score

print('Silhouette Score for PCA clustering :', silhouette_score(pca_df, kmeans_label_alotted))

print('Silhouette Score for Scaled Dataset without PCA: ',silhouette_score(df_scaled, label_alotted))

"""2*. Spectral CLustering

Applying the spectral clustering without PCA
"""

import numpy as np
import pandas as pd
from sklearn.cluster import SpectralClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Create a spectral clustering object
n_clusters = 4 # Choose the number of clusters you want
spec_cluster = SpectralClustering(n_clusters=n_clusters, affinity='rbf')

# Fit the data to the model
spec_cluster.fit(df_scaled)

# Get the labels for each data point
labels_spectral = spec_cluster.labels_

print(labels_spectral)
# Get the silhouette score
silhouette_avg = silhouette_score(df_scaled, spec_cluster.labels_)

# Print the silhouette score
print("The average silhouette score is :", silhouette_avg)

#Visualizing
plt.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], c=labels_spectral, cmap='viridis')
plt.show()

"""Applying the spectral clustering on PCA data"""

import numpy as np
import pandas as pd
from sklearn.cluster import SpectralClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Create a spectral clustering object
n_clusters = 4 # Choose the number of clusters you want
spec_cluster = SpectralClustering(n_clusters=n_clusters, affinity='rbf')

# Fit the data to the model
spec_cluster.fit(pca_df)

# Get the labels for each data point
labels_spectral = spec_cluster.labels_

print(labels_spectral)
# Get the silhouette score
silhouette_avg = silhouette_score(pca_df, spec_cluster.labels_)

# Print the silhouette score
print("The average silhouette score is :", silhouette_avg)

#Visualizing
plt.scatter(pca_df.iloc[:, 0], pca_df.iloc[:, 1], c=labels_spectral, cmap='viridis')
plt.show()

"""3*. BIRCH clustering method   --- branching factor, threshold, optional global clusterer.

Applying BIRCH clustering on the scaled data
"""

import numpy as np
import pandas as pd
from sklearn.cluster import Birch
from sklearn.metrics import silhouette_score

# Define the BIRCH clustering function
def birch_clustering(data, threshold, branching_factor, n_clusters):
    # Initialize the BIRCH clustering object
    birch = Birch(threshold=threshold, branching_factor=branching_factor, n_clusters=n_clusters)

    # Fit the BIRCH clustering model to the data
    birch.fit(data)

    # Return the cluster labels for each data point
    return birch.labels_


# Set the BIRCH clustering parameters
threshold = 0.5
branching_factor = 50
n_clusters = 4

# Run BIRCH clustering
labels_birch_scaled = birch_clustering(df_scaled, threshold, branching_factor, n_clusters)

# Calculate the silhouette score
score = silhouette_score(df_scaled, labels_birch_scaled)

#Print the labels
print(labels_birch_scaled)

# Print the silhouette score
print("Silhouette Score:", score)

#Viusalising the plot
import numpy as np
import pandas as pd
from sklearn.cluster import Birch
import matplotlib.pyplot as plt

# Plot the clustering
plt.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], c=labels_birch_scaled, cmap='viridis')
plt.show()

"""Applying the Birch clustering on PCA data

"""

import numpy as np
import pandas as pd
from sklearn.cluster import Birch
from sklearn.metrics import silhouette_score

# Define the BIRCH clustering function
def birch_clustering(data, threshold, branching_factor, n_clusters):
    # Initialize the BIRCH clustering object
    birch = Birch(threshold=threshold, branching_factor=branching_factor, n_clusters=n_clusters)

    # Fit the BIRCH clustering model to the data
    birch.fit(data)

    # Return the cluster labels for each data point
    return birch.labels_


# Set the BIRCH clustering parameters
threshold = 0.5
branching_factor = 50
n_clusters = 4

# Run BIRCH clustering
labels_birch_pca = birch_clustering(pca_df, threshold, branching_factor, n_clusters)

# Calculate the silhouette score
score = silhouette_score(pca_df, labels_birch_pca)

#Print the labels
print(labels_birch_pca)

# Print the silhouette score
print("Silhouette Score:", score)

# Plot the clustering
plt.scatter(pca_df.iloc[:, 0], pca_df.iloc[:, 1], c=labels_birch_pca, cmap='viridis')
plt.show()

"""2. Hierarchial Clustering:"""

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

Z = linkage(pca_df, method='ward', metric='euclidean')
plt.figure(figsize=(17,5))
plt.title('Dendrogram')
dendrogram(Z)
plt.show()

from sklearn.cluster import AgglomerativeClustering

agg_model = AgglomerativeClustering(n_clusters=4)
agg_model.fit(pca_df)
label_alotted = agg_model.labels_

print("The Assigned labels of the clusters:")
print(label_alotted)

# Getting the country list
Acl1 = []
Acl2 = []
Acl3 = []
Acl4 = []
for i in range(len(label_alotted)):
  if(label_alotted[i]==0):
    Acl1.append(country_list[i])
  if(label_alotted[i]==1):
    Acl2.append(country_list[i])
  if(label_alotted[i]==2):
    Acl3.append(country_list[i])
  if(label_alotted[i]==3):
    Acl4.append(country_list[i])

print("Countries in cluster 1",Acl1 )
print("Countries in cluster 2",Acl2 )
print("Countries in cluster 3",Acl3 )
print("Countries in cluster 4",Acl4 )

cnt0 = count_label(label_alotted, 0)
cnt1 = count_label(label_alotted, 1)
cnt2 = count_label(label_alotted, 2)
cnt3 = count_label(label_alotted, 3)

tot_cnt = cnt0 + cnt1 + cnt2 + cnt3
print(cnt0, cnt1, cnt2, cnt3)
print(cnt0*100/tot_cnt, cnt1*100/tot_cnt, cnt2*100/tot_cnt, cnt3*100/tot_cnt)

print('Silhouette Score for Agglomerative clustering for PCA dataset :', silhouette_score(pca_df, label_alotted))

colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'pink']
plt.figure(figsize=(8, 6))
for i in range(4):
    x_arr = pca_df.iloc[label_alotted == i, 0]
    y_arr = pca_df.iloc[label_alotted == i, 1]
    plt.scatter(x_arr, y_arr, s=30, c=colors[i])

plt.title('Agglomerative Clustering on PCA dataset:')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

agg_model_scaled = AgglomerativeClustering(n_clusters=4)
agg_model_scaled.fit(df_scaled)
label_alotted_scaled = agg_model_scaled.labels_

print("The Assigned labels of the clusters on scaled dataset:")
print(label_alotted_scaled)

print('Silhouette Score for Agglomerative clustering for scaled data :', silhouette_score(df_scaled, label_alotted_scaled))

"""3. DBSCAN Clustering:"""

from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN

# for tuning the epsilon parameter for DBSCAN
ini_neigh = 2
nei_model = NearestNeighbors(n_neighbors=ini_neigh)
neighbors = nei_model.fit(pca_df)
dist, ind = neighbors.kneighbors(pca_df)
dist = (np.sort(dist, axis = 0))[:,1]
plt.title('Plot for determining value of epsilon hyperparameter:')
plt.plot(dist)

# taking epsilon = 1.5, and min_samples = 4

dbscan = DBSCAN(eps = 1.5, min_samples = 4)
dbscan.fit(pca_df)
label = dbscan.labels_

print("The Assigned labels of the clusters:")
print(label)

# In this code , we are getting the core cluster data and te outliers

# Getting the country list
outlier = []
Core = []
for i in range(len(label)):
  if(label[i]==-1):
    outlier.append(country_list[i])
  if(label[i]==0):
    Core.append(country_list[i])


print("Countries in Outlier",outlier )
print("Countries in Core",Core )

def count_labels(arr, cla):
  cnt = 0
  for i in arr:
    if i == cla:
      cnt += 1
  return cnt

cnt0 = count_labels(label, 0)
cnt1 = count_labels(label, -1)

tot_cnt = cnt0 + cnt1
print(cnt0, cnt1)
print(cnt0*100/tot_cnt, cnt1*100/tot_cnt)

print('Silhouette Score for DBSCAN clustering :', silhouette_score(pca_df, label))

plt.figure(figsize=(8, 6))
col1 = pca_df['PC1']
col2 = pca_df['PC2']
plt.scatter(col1,col2,c=label)
plt.title('DBSCAN Clustering on PCA dataset:')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

"""We will choose to go with the KMeans Clustering as it is showing best results comparatively and visualization of clusters."""

df_new = pca_df.copy()
df_new['Country'] = country_names
df_new['Cluster_ID'] = kmeans_label_alotted
df_new

cnt0 = count_label(kmeans_label_alotted, 0)
cnt1 = count_label(kmeans_label_alotted, 1)
cnt2 = count_label(kmeans_label_alotted, 2)
cnt3 = count_label(kmeans_label_alotted, 3)

tot_cnt = cnt0 + cnt1 + cnt2 + cnt3
print("Number of Countries in the Cluster 1 (id = 0): ", cnt0)
print("Number of Countries in the Cluster 2 (id = 1): ", cnt1)
print("Number of Countries in the Cluster 3 (id = 2): ", cnt2)
print("Number of Countries in the Cluster 4 (id = 3): ", cnt3)
print("\n")
print("Percentage of Countries in the Cluster 1 (id = 0): ", cnt0*100/tot_cnt, "%")
print("Percentage of Countries in the Cluster 1 (id = 0): ", cnt1*100/tot_cnt, "%")
print("Percentage of Countries in the Cluster 1 (id = 0): ", cnt2*100/tot_cnt, "%")
print("Percentage of Countries in the Cluster 1 (id = 0): ", cnt3*100/tot_cnt, "%")

print("cluster 1 obtained by k_means" ,cl1)
print("cluster 2 obtained by k_means" ,cl2)
print("cluster 3 obtained by k_means" ,cl3)
print("cluster 4 obtained by k_means" ,cl4)

"""Analysing the cluster obtained

"""

# Getting avg function

def avg(cluster_lst, feature_lst):
  t = 0
  for i in cluster_lst:
    j = country_list.index(i)
    t = t + feature_lst[j]
  avg = t/len(cluster_lst)
  return avg

# Analysis avg life expectancy
lyf_lst = df_original['life_expec'].to_numpy()
A = avg(cl1, lyf_lst)
B = avg(cl2, lyf_lst)
C = avg(cl3, lyf_lst)
D = avg(cl4, lyf_lst)
print("Avg life expectancy of cluster 1 countries is:", A)
print("Avg life expectancy of cluster 2 countries is:", B)
print("Avg life expectancy of cluster 3 countries is:", C)
print("Avg life expectancy of cluster 4 countries is:", D)

# Analysis avg GDP
gdp_lst = df_original['gdpp'].to_numpy()
GDA = avg(cl1, gdp_lst)
GDB = avg(cl2, gdp_lst)
GDC = avg(cl3, gdp_lst)
GDD = avg(cl4, gdp_lst)
print("Avg GDP of cluster 1 countries is:", GDA)
print("Avg GDP of cluster 2 countries is:", GDB)
print("Avg GDP of cluster 3 countries is:", GDC)
print("Avg GDP of cluster 4 countries is:", GDD)

# Analyzing child mortality rate
mr_lst = df_original['child_mort'].to_numpy()
GA = avg(cl1, mr_lst)
GB = avg(cl2, mr_lst)
GC = avg(cl3, mr_lst)
GD = avg(cl4, mr_lst)
print("Avg child mortality rate of cluster 1 countries is:", GA)
print("Avg child mortality rate of cluster 2 countries is:", GB)
print("Avg child mortality rate of cluster 3 countries is:", GC)
print("Avg child mortality rate of cluster 4 countries is:", GD)

# Analyzing the health factor
h_lst = df_original['health'].to_numpy()
A = avg(cl1, h_lst)
B = avg(cl2, h_lst)
C = avg(cl3, h_lst)
D = avg(cl4, h_lst)
print("Avg health of cluster 1 countries is:", A)
print("Avg health of cluster 2 countries is:", B)
print("Avg health of cluster 3 countries is:", C)
print("Avg health of cluster 4 countries is:", D)

"""On the avaerage life quality features of the clusters and observing the indivial data for the rest, we can see that :

Cluster 1 --> Most developed countries

Cluster 2 --> Under-developed (Help needed)

Cluster 3 --> Most backward in terms of health and finance parameters

Cluster 4 --> Developing countries (Might Need Help)
"""

# We have seen that the clustering order is in the same order as the variables GDA, GDB, GDC & GDD
lst = [GDA, GDB , GDC, GDD]
from collections import OrderedDict
import numpy as np

dict = {1: GDA, 2: GDB, 3: GDC, 4: GDD}
print(dict)

keys = list(dict.keys())
values = list(dict.values())
sorted_value_index = np.argsort(values)
sorted_dict = {keys[i]: values[i] for i in sorted_value_index}

print(sorted_dict)

clist = []
for keys, value in sorted_dict.items():
   clist.append(keys)
print(clist)

cl_val = [cl1,cl2,cl3,cl4]
print("Need Help" ,cl_val[clist[1]-1])
print("Most Help Needed" ,cl_val[clist[0]-1])
print("No Help Needed" ,cl_val[clist[3]-1])
print("Might Need Help" ,cl_val[clist[2]-1])

df_new['Cluster_ID'].loc[df_new['Cluster_ID'] == (clist[1]-1)] = 'Need Help'
df_new['Cluster_ID'].loc[df_new['Cluster_ID'] == (clist[0]-1)] = 'Most Help Needed'
df_new['Cluster_ID'].loc[df_new['Cluster_ID'] == (clist[3]-1)] = 'No Help Needed'
df_new['Cluster_ID'].loc[df_new['Cluster_ID'] == (clist[2]-1)] = 'Might Help Needed'

fig = px.choropleth(df_new[['Country','Cluster_ID']],
                    locationmode = 'country names',
                    locations = 'Country',
                    title = 'Needed Help Per Country (World)',
                    color = df_new['Cluster_ID'],
                    color_discrete_map = {'Most Help Needed':'Red',
                                        'No Help Needed':'Green',

                                        'Need Help':'Yellow',
                                        'Might Help Needed':'Blue'}
                   )
fig.update_geos(fitbounds = "locations", visible = True)
fig.update_layout(legend_title_text = 'Labels',legend_title_side = 'top',title_pad_l = 260,title_y = 0.86)
fig.show(engine = 'kaleido')

sns.pairplot(df_new, hue = 'Cluster_ID')

# Now in the propbelm statement, we are given the total amount of 10 million dollars, so to make the fianl decision for the investor easy, we can give him the final data
#of the needed country is sorted order, i.e. country which require the help as first and so on....

print(cl_val[clist[0]-1])

#gdp_lst , country_list
help_lst_gdp = []
for i in ((cl_val[clist[0]-1])):
  j = country_list.index(i)
  help_lst_gdp.append(gdp_lst[j])

print(help_lst_gdp)

def sort_acc(list1, list2):
	a = list(set(list2))
	a.sort()
	res = []
	for i in a:
		for j in range(0, len(list2)):
			if(list2[j] == i):
				res.append(list1[j])
	return res

a = help_lst_gdp
b = cl_val[clist[0]-1]
arr_ = sort_acc(b, a)
a.sort()

print("Decreasing order of the help need staring from most help needed country: ", arr_)
print("Corresponding gdp list", a)

# If the invsrting body needs to distribute the money among top n most needing g=countries then the unequal distribution of money according to the need is is given below:

def amount_dist(n):
  dic = {}
  s = 0
  for i in range(0, n):
    s += a[i]
  for i in range(0, n):
    a[i] = a[i]/s
  for i in range(0, n):
    a[i] = 1 - a[i]
  newsum = 0
  for i in range(0, n):
    newsum += a[i]
  for i in range(0, n):
    val = (a[i]/newsum)*10000000
    key = arr_[i]
    dic[key]=val
  return dic

#Amount in dollars
#Let the investment body feels the need to invest in top 7 needy countries then n=7 and the money distribution pattern is as follows:
dist = amount_dist(7)
print(dist)

"""Conclusions:
There are 47 countries belonging to cluster 1 and they are the most underdeveloped countries. This can be seen through the high child mortality, inflation as well as very low life expectancy, income, health index and gdp. These countries need the most help.

There are only 3 countries belonging to cluster 3. These are the most developed countries with very high income, life expectancy and gdp.

There are 30 countries belonging to cluster 2 and they are also developed countries. Their lower income and gdp is what sets them apart from the most developed countries of cluster 3.

There are 87 countries belonging to cluster 0 and constitute over 50% of the total countries. These can all be categorized as 'developing' as they are one step above the undeveloped countries but still have relatively low income, gdp and life expectancy, as well as higher child mortality and inflation. These countries may require help.
"""